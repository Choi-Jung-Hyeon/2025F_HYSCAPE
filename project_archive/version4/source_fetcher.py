# source_fetcher.py (v3.0)
"""
ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ê¸°ì‚¬ ëª©ë¡ì„ ìˆ˜ì§‘í•˜ëŠ” ëª¨ë“ˆ
- ì›¹ í¬ë¡¤ë§ (ì›”ê°„ìˆ˜ì†Œê²½ì œ ë“±)
- RSS í”¼ë“œ (Hydrogen Central ë“±)
- ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
- êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ (NEW!)
"""

import requests
from bs4 import BeautifulSoup
import feedparser
import time
from datetime import datetime
from config import (
    NEWS_SOURCES, 
    NAVER_KEYWORDS, 
    GOOGLE_KEYWORDS,
    MAX_ARTICLES_PER_SOURCE,
    MAX_NAVER_PER_KEYWORD,
    MAX_GOOGLE_PER_KEYWORD,
    FAILED_SOURCES_LOG
)

# ========================================
# ì‹¤íŒ¨í•œ ì†ŒìŠ¤ ë¡œê¹…
# ========================================
def log_failed_source(source_name, reason):
    """ì‹¤íŒ¨í•œ ì†ŒìŠ¤ë¥¼ ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡"""
    with open(FAILED_SOURCES_LOG, 'a', encoding='utf-8') as f:
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        f.write(f"[{timestamp}] {source_name}: {reason}\n")
    print(f"  âš ï¸  {source_name} ì‹¤íŒ¨: {reason}")

# ========================================
# 1. ì›¹ ìŠ¤í¬ë˜í•‘ Fetcher
# ========================================
class WebFetcher:
    """ì¼ë°˜ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘"""
    
    def __init__(self, source_name, url):
        self.source_name = source_name
        self.url = url
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def fetch_articles(self, limit=5):
        """ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘"""
        try:
            response = requests.get(self.url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            articles = []
            
            # ì›”ê°„ìˆ˜ì†Œê²½ì œ ì „ìš© íŒŒì‹±
            if "h2news.kr" in self.url:
                articles = self._parse_h2news(soup)
            
            # H2 View íŒŒì‹±
            elif "h2-view.com" in self.url:
                articles = self._parse_h2view(soup)
            
            # ê¸°íƒ€ ì‚¬ì´íŠ¸ ì¼ë°˜ íŒŒì‹±
            else:
                articles = self._parse_generic(soup)
            
            articles = articles[:limit]
            print(f"  âœ… {self.source_name}: {len(articles)}ê°œ ìˆ˜ì§‘")
            return articles
            
        except requests.exceptions.HTTPError as e:
            log_failed_source(self.source_name, f"HTTP {e.response.status_code}")
            return []
        except Exception as e:
            log_failed_source(self.source_name, str(e))
            return []
    
    def _parse_h2news(self, soup):
        """ì›”ê°„ìˆ˜ì†Œê²½ì œ íŒŒì‹±"""
        articles = []
        for item in soup.select('div.card-body'):
            try:
                title_tag = item.select_one('a.card-title')
                if title_tag:
                    title = title_tag.text.strip()
                    url = "https://www.h2news.kr" + title_tag['href']
                    
                    articles.append({
                        'title': title,
                        'url': url,
                        'source': self.source_name
                    })
            except:
                continue
        return articles
    
    def _parse_h2view(self, soup):
        """H2 View íŒŒì‹±"""
        articles = []
        for item in soup.select('article.post'):
            try:
                title_tag = item.select_one('h2.entry-title a')
                if title_tag:
                    title = title_tag.text.strip()
                    url = title_tag['href']
                    
                    articles.append({
                        'title': title,
                        'url': url,
                        'source': self.source_name
                    })
            except:
                continue
        return articles
    
    def _parse_generic(self, soup):
        """ì¼ë°˜ ì‚¬ì´íŠ¸ íŒŒì‹±"""
        articles = []
        
        # article íƒœê·¸ ì°¾ê¸°
        for item in soup.find_all(['article', 'div'], class_=['post', 'article', 'news-item'])[:10]:
            try:
                title_tag = item.find(['h1', 'h2', 'h3', 'h4'], class_=['title', 'headline'])
                link_tag = title_tag.find('a') if title_tag else item.find('a')
                
                if link_tag:
                    title = link_tag.text.strip()
                    url = link_tag.get('href', '')
                    
                    if not url.startswith('http'):
                        url = self.url.rstrip('/') + '/' + url.lstrip('/')
                    
                    articles.append({
                        'title': title,
                        'url': url,
                        'source': self.source_name
                    })
            except:
                continue
        
        return articles

# ========================================
# 2. RSS Fetcher
# ========================================
class RSSFetcher:
    """RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘"""
    
    def __init__(self, source_name, url):
        self.source_name = source_name
        self.url = url
    
    def fetch_articles(self, limit=5):
        """RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
        try:
            feed = feedparser.parse(self.url)
            
            if not feed.entries:
                log_failed_source(self.source_name, "No entries in RSS feed")
                return []
            
            articles = []
            for entry in feed.entries[:limit]:
                articles.append({
                    'title': entry.get('title', 'No title'),
                    'url': entry.get('link', ''),
                    'source': self.source_name
                })
            
            print(f"  âœ… {self.source_name}: {len(articles)}ê°œ ìˆ˜ì§‘")
            return articles
            
        except Exception as e:
            log_failed_source(self.source_name, str(e))
            return []

# ========================================
# 3. ë„¤ì´ë²„ ë‰´ìŠ¤ Fetcher
# ========================================
class NaverNewsFetcher:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰"""
    
    def __init__(self):
        self.source_name = "ë„¤ì´ë²„ë‰´ìŠ¤"
        self.base_url = "https://search.naver.com/search.naver"
    
    def fetch_articles(self, keywords, max_per_keyword=3):
        """í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ê²€ìƒ‰"""
        all_articles = []
        
        for keyword in keywords:
            try:
                params = {
                    'where': 'news',
                    'query': keyword,
                    'sort': '1',  # ìµœì‹ ìˆœ
                    'start': 1
                }
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
                }
                
                response = requests.get(
                    self.base_url,
                    params=params,
                    headers=headers,
                    timeout=10
                )
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                articles = []
                
                # ë„¤ì´ë²„ ë‰´ìŠ¤ HTML êµ¬ì¡° (2025ë…„ ë²„ì „)
                # ë°©ë²• 1: news_area í´ë˜ìŠ¤
                news_items = soup.select('div.news_area')
                
                if not news_items:
                    # ë°©ë²• 2: ë‹¤ë¥¸ ì„ íƒì ì‹œë„
                    news_items = soup.select('li.bx')
                
                for item in news_items[:max_per_keyword]:
                    try:
                        # ì œëª©ê³¼ URL ì¶”ì¶œ
                        title_tag = item.select_one('a.news_tit')
                        if not title_tag:
                            title_tag = item.select_one('a[class*="news"]')
                        
                        if title_tag:
                            title = title_tag.get_text(strip=True)
                            url = title_tag.get('href', '')
                            
                            if title and url:
                                articles.append({
                                    'title': title,
                                    'url': url,
                                    'source': f"{self.source_name}({keyword})"
                                })
                    except:
                        continue
                
                all_articles.extend(articles)
                
                if articles:
                    print(f"  âœ… ë„¤ì´ë²„({keyword}): {len(articles)}ê°œ")
                else:
                    print(f"  âš ï¸  ë„¤ì´ë²„({keyword}): 0ê°œ (HTML êµ¬ì¡° ë³€ê²½ ê°€ëŠ¥ì„±)")
                
                time.sleep(1)  # ìš”ì²­ ê°„ê²© (ë„¤ì´ë²„ ì°¨ë‹¨ ë°©ì§€)
                
            except Exception as e:
                log_failed_source(f"ë„¤ì´ë²„({keyword})", str(e))
        
        return all_articles

# ========================================
# 4. êµ¬ê¸€ ë‰´ìŠ¤ Fetcher (NEW!)
# ========================================
class GoogleNewsFetcher:
    """êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰"""
    
    def __init__(self):
        self.source_name = "êµ¬ê¸€ë‰´ìŠ¤"
        self.base_url = "https://news.google.com/rss/search"
    
    def fetch_articles(self, keywords, max_per_keyword=3):
        """í‚¤ì›Œë“œë³„ êµ¬ê¸€ ë‰´ìŠ¤ RSS ê²€ìƒ‰"""
        all_articles = []
        
        for keyword in keywords:
            try:
                # êµ¬ê¸€ ë‰´ìŠ¤ RSS URL
                params = {
                    'q': keyword,
                    'hl': 'en-US',
                    'gl': 'US',
                    'ceid': 'US:en'
                }
                
                response = requests.get(
                    self.base_url,
                    params=params,
                    timeout=10
                )
                response.raise_for_status()
                
                # RSS íŒŒì‹±
                feed = feedparser.parse(response.content)
                articles = []
                
                for entry in feed.entries[:max_per_keyword]:
                    # êµ¬ê¸€ ë‰´ìŠ¤ URLì—ì„œ ì‹¤ì œ ê¸°ì‚¬ URL ì¶”ì¶œ
                    actual_url = self._extract_actual_url(entry.get('link', ''))
                    
                    articles.append({
                        'title': entry.get('title', 'No title'),
                        'url': actual_url,
                        'source': f"{self.source_name}({keyword})"
                    })
                
                all_articles.extend(articles)
                print(f"  âœ… êµ¬ê¸€({keyword}): {len(articles)}ê°œ")
                time.sleep(0.5)  # ìš”ì²­ ê°„ê²©
                
            except Exception as e:
                log_failed_source(f"êµ¬ê¸€({keyword})", str(e))
        
        return all_articles
    
    def _extract_actual_url(self, google_url):
        """
        êµ¬ê¸€ ë‰´ìŠ¤ redirect URLì—ì„œ ì‹¤ì œ ê¸°ì‚¬ URL ì¶”ì¶œ
        ì˜ˆ: https://news.google.com/rss/articles/... â†’ ì‹¤ì œ URL
        """
        try:
            # redirect ë”°ë¼ê°€ê¸°
            response = requests.head(google_url, allow_redirects=True, timeout=5)
            return response.url
        except:
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ URL ë°˜í™˜
            return google_url

# ========================================
# 5. Fetcher Manager
# ========================================
class FetcherManager:
    """ëª¨ë“  Fetcherë¥¼ ê´€ë¦¬í•˜ëŠ” ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.fetchers = []
    
    def add_fetcher(self, fetcher):
        """Fetcher ì¶”ê°€"""
        self.fetchers.append(fetcher)
    
    def fetch_all_articles(self, limit_per_source=5):
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
        all_articles = []
        
        for fetcher in self.fetchers:
            if hasattr(fetcher, 'fetch_articles'):
                # NaverNewsFetcher, GoogleNewsFetcherëŠ” keywords ì¸ì í•„ìš”
                if isinstance(fetcher, NaverNewsFetcher):
                    articles = fetcher.fetch_articles(
                        NAVER_KEYWORDS, 
                        MAX_NAVER_PER_KEYWORD
                    )
                elif isinstance(fetcher, GoogleNewsFetcher):
                    articles = fetcher.fetch_articles(
                        GOOGLE_KEYWORDS,
                        MAX_GOOGLE_PER_KEYWORD
                    )
                else:
                    articles = fetcher.fetch_articles(limit_per_source)
                
                all_articles.extend(articles)
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_articles = []
        for article in all_articles:
            if article['url'] not in seen_urls:
                seen_urls.add(article['url'])
                unique_articles.append(article)
        
        print(f"\nğŸ“Š ì´ {len(unique_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ (ì¤‘ë³µ ì œê±° í›„)")
        return unique_articles

# ========================================
# 6. Config ê¸°ë°˜ Fetcher ìƒì„±
# ========================================
def create_fetchers_from_config():
    """config.py ê¸°ë°˜ìœ¼ë¡œ Fetcher Manager ìƒì„±"""
    manager = FetcherManager()
    
    # NEWS_SOURCESì—ì„œ Fetcher ìƒì„±
    for source_name, info in NEWS_SOURCES.items():
        if info.get('status') == 'active' or info.get('status') == 'testing':
            if info['type'] == 'web':
                manager.add_fetcher(WebFetcher(source_name, info['url']))
            elif info['type'] == 'rss':
                manager.add_fetcher(RSSFetcher(source_name, info['url']))
    
    # ë„¤ì´ë²„ ë‰´ìŠ¤ ì¶”ê°€
    manager.add_fetcher(NaverNewsFetcher())
    
    # êµ¬ê¸€ ë‰´ìŠ¤ ì¶”ê°€ (NEW!)
    manager.add_fetcher(GoogleNewsFetcher())
    
    return manager

# ========================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ========================================
if __name__ == "__main__":
    print("=" * 70)
    print("ğŸ§ª Source Fetcher v3.0 í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    manager = create_fetchers_from_config()
    articles = manager.fetch_all_articles(limit_per_source=MAX_ARTICLES_PER_SOURCE)
    
    print(f"\nâœ… ì´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
    for i, article in enumerate(articles[:5], 1):
        print(f"{i}. [{article['source']}] {article['title'][:50]}...")