"""
K-Startup í¬ë¡¤ëŸ¬ êµ¬í˜„ì²´ (ë””ë²„ê¹… ëª¨ë“œ ì¶”ê°€)
https://www.k-startup.go.kr/ ì‚¬ì—…ê³µê³  í¬ë¡¤ë§
"""

import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from datetime import datetime
import re
import time

from scrapers.base_scraper import BaseScraper


class KStartupScraper(BaseScraper):
    """
    K-Startup (ì°½ì—…ë„·) ì‚¬ì—…ê³µê³  í¬ë¡¤ëŸ¬
    
    í•„í„° ì „ëµ: Type B (ì§€ì› ì¤‘ì‹¬)
    - ê¸°ìˆ  í‚¤ì›Œë“œ OR (ì§€ì› í‚¤ì›Œë“œ AND ìê²© í‚¤ì›Œë“œ)
    """
    
    def __init__(self, config: Dict):
        super().__init__(config, site_name='k_startup')
        
        # K-Startup íŠ¹í™” ì„¤ì •
        self.base_url = "https://www.k-startup.go.kr"
        self.list_url = f"{self.base_url}/web/contents/biznotify.do"
        self.max_pages = 3  # ìµœê·¼ 3í˜ì´ì§€ë§Œ í¬ë¡¤ë§
        
        # ğŸ› ë””ë²„ê¹… ëª¨ë“œ ì„¤ì •
        self.debug_mode = True  # HTML íŒŒì¼ ì €ì¥ ì—¬ë¶€
        
    def fetch_announcements(self) -> List[Dict]:
        """
        K-Startup ê³µê³  ëª©ë¡ í˜ì´ì§€ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
        
        Returns:
            List[Dict]: ì›ë³¸ ê³µê³  ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        all_announcements = []
        
        for page in range(1, self.max_pages + 1):
            self.logger.info(f"í˜ì´ì§€ {page}/{self.max_pages} í¬ë¡¤ë§ ì¤‘...")
            
            try:
                # í˜ì´ì§€ ìš”ì²­
                params = {
                    'schM': 'list',
                    'page': page,
                    'schBzType': '',  # ì‚¬ì—… ìœ í˜• (ì „ì²´)
                    'schStts': 'R',   # ëª¨ì§‘ì¤‘(R) / ë§ˆê°(D)
                }
                
                response = requests.get(
                    self.list_url,
                    params=params,
                    headers=self.get_headers(),
                    timeout=10
                )
                response.raise_for_status()
                
                # ğŸ› ë””ë²„ê¹…: HTML íŒŒì¼ë¡œ ì €ì¥
                if self.debug_mode and page == 1:
                    debug_file = f"debug_kstartup_page{page}.html"
                    with open(debug_file, 'w', encoding='utf-8') as f:
                        f.write(response.text)
                    self.logger.info(f"ğŸ› ë””ë²„ê·¸ HTML ì €ì¥: {debug_file}")
                
                # HTML íŒŒì‹±
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ê³µê³  ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
                announcements = self._parse_list_page(soup)
                all_announcements.extend(announcements)
                
                self.logger.info(f"í˜ì´ì§€ {page}: {len(announcements)}ê°œ ê³µê³  ìˆ˜ì§‘")
                
                # ì„œë²„ ë¶€í•˜ ë°©ì§€
                time.sleep(1)
                
            except Exception as e:
                self.logger.error(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
                continue
        
        return all_announcements
    
    def _parse_list_page(self, soup: BeautifulSoup) -> List[Dict]:
        """
        ëª©ë¡ í˜ì´ì§€ HTML íŒŒì‹±
        
        Args:
            soup: BeautifulSoup ê°ì²´
            
        Returns:
            List[Dict]: ê³µê³  ì›ë³¸ ë°ì´í„°
        """
        announcements = []
        
        # ğŸ” ì—¬ëŸ¬ ê°€ëŠ¥í•œ HTML êµ¬ì¡° ì‹œë„
        selectors = [
            'table.board-list tbody tr',           # í…Œì´ë¸” í˜•ì‹ 1
            'table tbody tr',                       # í…Œì´ë¸” í˜•ì‹ 2
            'div.board-list ul li',                 # ë¦¬ìŠ¤íŠ¸ í˜•ì‹ 1
            'ul.notice-list li',                    # ë¦¬ìŠ¤íŠ¸ í˜•ì‹ 2
            'div.list-wrap div.item',               # ì¹´ë“œ í˜•ì‹ 1
            'div.notice-item',                      # ì¹´ë“œ í˜•ì‹ 2
        ]
        
        rows = []
        for selector in selectors:
            rows = soup.select(selector)
            if rows:
                self.logger.info(f"âœ… ë§¤ì¹­ëœ ì…€ë ‰í„°: '{selector}' ({len(rows)}ê°œ í•­ëª©)")
                break
        
        if not rows:
            self.logger.warning("âš ï¸ ê³µê³  ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. HTML êµ¬ì¡° í™•ì¸ í•„ìš”")
            # ğŸ› ë””ë²„ê¹…: í˜ì´ì§€ êµ¬ì¡° ì¶œë ¥
            self._debug_html_structure(soup)
            return []
        
        for row in rows:
            try:
                # ì œëª©ê³¼ URL ì¶”ì¶œ (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)
                title_elem = (
                    row.select_one('a') or 
                    row.select_one('.title a') or 
                    row.select_one('td a')
                )
                
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                
                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                url = title_elem.get('href', '')
                if url and not url.startswith('http'):
                    url = self.base_url + url
                
                # ë§ˆê°ì¼ ì¶”ì¶œ
                deadline = self._extract_deadline(row)
                
                # ì£¼ê´€ê¸°ê´€ ì¶”ì¶œ
                organization = self._extract_organization(row)
                
                announcements.append({
                    'title': title,
                    'url': url,
                    'deadline': deadline,
                    'organization': organization,
                    'raw_html': str(row)[:500],  # ì²˜ìŒ 500ìë§Œ ì €ì¥
                })
                
            except Exception as e:
                self.logger.warning(f"í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                continue
        
        return announcements
    
    def _debug_html_structure(self, soup: BeautifulSoup):
        """
        HTML êµ¬ì¡° ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        
        Args:
            soup: BeautifulSoup ê°ì²´
        """
        self.logger.info("=" * 60)
        self.logger.info("ğŸ› HTML êµ¬ì¡° ë””ë²„ê¹…")
        self.logger.info("=" * 60)
        
        # ì£¼ìš” íƒœê·¸ ê°œìˆ˜ í™•ì¸
        tags_to_check = ['table', 'ul', 'div.list', 'div.board', 'article', 'section']
        for tag in tags_to_check:
            count = len(soup.select(tag))
            if count > 0:
                self.logger.info(f"  {tag}: {count}ê°œ")
        
        # ë§í¬ ê°œìˆ˜ í™•ì¸
        links = soup.find_all('a', href=True)
        self.logger.info(f"  ì „ì²´ ë§í¬(<a>): {len(links)}ê°œ")
        
        # ì²« ë²ˆì§¸ ë§í¬ ìƒ˜í”Œ
        if links:
            sample = links[0]
            self.logger.info(f"  ë§í¬ ìƒ˜í”Œ: {sample.get_text(strip=True)[:50]}")
        
        self.logger.info("=" * 60)
        self.logger.info("ğŸ’¡ debug_kstartup_page1.html íŒŒì¼ì„ ì—´ì–´ì„œ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”!")
        self.logger.info("=" * 60)
    
    def _extract_deadline(self, element) -> str:
        """
        ë§ˆê°ì¼ ì¶”ì¶œ ë° í‘œì¤€í™”
        
        Args:
            element: HTML ìš”ì†Œ
            
        Returns:
            str: YYYY-MM-DD í˜•ì‹ ë‚ ì§œ
        """
        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì…€ë ‰í„° ì‹œë„
        selectors = [
            '.date', '.deadline', '.period', '.end-date',
            'td:nth-child(4)', 'td:nth-child(5)',
            'span.date', 'div.date'
        ]
        
        date_elem = None
        for selector in selectors:
            date_elem = element.select_one(selector)
            if date_elem:
                break
        
        if date_elem:
            date_text = date_elem.get_text(strip=True)
            
            # ë‚ ì§œ íŒ¨í„´ ì¶”ì¶œ (YYYY-MM-DD ë˜ëŠ” YYYY.MM.DD)
            match = re.search(r'(\d{4})[-.](\d{2})[-.](\d{2})', date_text)
            if match:
                return f"{match.group(1)}-{match.group(2)}-{match.group(3)}"
        
        return "ë¯¸ì •"
    
    def _extract_organization(self, element) -> str:
        """
        ì£¼ê´€ê¸°ê´€ ì¶”ì¶œ
        
        Args:
            element: HTML ìš”ì†Œ
            
        Returns:
            str: ê¸°ê´€ëª…
        """
        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì…€ë ‰í„° ì‹œë„
        selectors = [
            '.organ', '.organization', '.agency', '.dept',
            'td:nth-child(2)', 'td:nth-child(3)',
            'span.organ', 'div.organ'
        ]
        
        org_elem = None
        for selector in selectors:
            org_elem = element.select_one(selector)
            if org_elem:
                break
        
        if org_elem:
            return org_elem.get_text(strip=True)
        
        return "ë¯¸í™•ì¸"
    
    def parse_announcement(self, raw_data: Dict) -> Optional[Dict]:
        """
        ì›ë³¸ ê³µê³  ë°ì´í„°ë¥¼ í‘œì¤€ í¬ë§·ìœ¼ë¡œ ë³€í™˜
        
        Args:
            raw_data: fetch_announcements()ì—ì„œ ë°˜í™˜ëœ ì›ë³¸ ë°ì´í„°
            
        Returns:
            Optional[Dict]: í‘œì¤€í™”ëœ ê³µê³  ì •ë³´
        """
        try:
            # ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ (ì„ íƒì‚¬í•­)
            description = self._fetch_detail_page(raw_data['url'])
            
            announcement = {
                'title': raw_data['title'],
                'url': raw_data['url'],
                'deadline': raw_data['deadline'],
                'organization': raw_data['organization'],
                'description': description or raw_data['title'],
                'tags': self._extract_tags(raw_data['title'], description),
                'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            }
            
            # ìœ íš¨ì„± ê²€ì‚¬
            if self.validate_announcement(announcement):
                return announcement
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"ê³µê³  íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _fetch_detail_page(self, url: str) -> Optional[str]:
        """
        ìƒì„¸ í˜ì´ì§€ì—ì„œ ê³µê³  ë‚´ìš© ìˆ˜ì§‘
        
        Args:
            url: ìƒì„¸ í˜ì´ì§€ URL
            
        Returns:
            Optional[str]: ê³µê³  ìƒì„¸ ë‚´ìš©
        """
        try:
            response = requests.get(url, headers=self.get_headers(), timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)
            selectors = [
                '.content', '.view-content', '#content',
                '.article-content', '.detail-content',
                'div.cont', 'div.view'
            ]
            
            content = None
            for selector in selectors:
                content = soup.select_one(selector)
                if content:
                    break
            
            if content:
                # HTML íƒœê·¸ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                text = content.get_text(separator=' ', strip=True)
                # ê³¼ë„í•œ ê³µë°± ì œê±°
                text = re.sub(r'\s+', ' ', text)
                return text[:2000]  # ì²˜ìŒ 2000ìë§Œ ì €ì¥
            
            return None
            
        except Exception as e:
            self.logger.warning(f"ìƒì„¸ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {url}")
            return None
    
    def _extract_tags(self, title: str, description: Optional[str]) -> List[str]:
        """
        ì œëª©ê³¼ ë‚´ìš©ì—ì„œ íƒœê·¸ ì¶”ì¶œ
        
        Args:
            title: ê³µê³  ì œëª©
            description: ê³µê³  ë‚´ìš©
            
        Returns:
            List[str]: íƒœê·¸ ë¦¬ìŠ¤íŠ¸
        """
        tags = []
        text = f"{title} {description or ''}"
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ íƒœê·¸ ìƒì„±
        tag_keywords = {
            'ìˆ˜ì†Œ': 'ìˆ˜ì†Œ',
            'ì—°ë£Œì „ì§€': 'ì—°ë£Œì „ì§€',
            'ë§ˆì¼€íŒ…': 'ë§ˆì¼€íŒ…ì§€ì›',
            'ìˆ˜ì¶œ': 'ê¸€ë¡œë²Œ',
            'ì„±ë‚¨': 'ì§€ì—­-ì„±ë‚¨',
            'ê²½ê¸°': 'ì§€ì—­-ê²½ê¸°',
            'ì°½ì—…': 'ì°½ì—…ì§€ì›',
        }
        
        for keyword, tag in tag_keywords.items():
            if keyword in text:
                tags.append(tag)
        
        return tags


# ìŠ¤í¬ë¦½íŠ¸ë¡œ ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
if __name__ == '__main__':
    import yaml
    import logging
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ì„¤ì • ë¡œë“œ
    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # í¬ë¡¤ëŸ¬ ì‹¤í–‰
    scraper = KStartupScraper(config)
    results = scraper.scrape()
    
    print(f"\n=== í¬ë¡¤ë§ ê²°ê³¼: {len(results)}ê°œ ê³µê³  ===")
    for i, announcement in enumerate(results[:3], 1):
        print(f"\n{i}. {announcement['title']}")
        print(f"   ë§ˆê°ì¼: {announcement['deadline']}")
        print(f"   URL: {announcement['url']}")