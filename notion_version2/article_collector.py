#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•œêµ­ìˆ˜ì†Œì—°í•©(H2HUB) PDF ë¸Œë¦¬í•‘ ìˆ˜ì§‘ ëª¨ë“ˆ
ì›¹ì‚¬ì´íŠ¸ì—ì„œ "ë¸Œë¦¬í•‘" í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²Œì‹œê¸€ì˜ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
"""

import logging
import time
from pathlib import Path
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse, parse_qs
import re

import requests
from bs4 import BeautifulSoup

import config

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class H2HubCollector:
    """
    í•œêµ­ìˆ˜ì†Œì—°í•©(H2HUB) ë¸Œë¦¬í•‘ ìˆ˜ì§‘ í´ë˜ìŠ¤
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. ê²Œì‹œíŒ í˜ì´ì§€ í¬ë¡¤ë§
    2. "ë¸Œë¦¬í•‘" í‚¤ì›Œë“œ í•„í„°ë§
    3. PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    """
    
    def __init__(self):
        """ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”"""
        self.session = requests.Session()
        self.session.headers.update(config.DEFAULT_HEADERS)
        self.download_dir = config.DOWNLOADS_DIR
        
        logger.info("H2HUB Collector ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ë‹¤ìš´ë¡œë“œ ê²½ë¡œ: {self.download_dir}")
    
    def collect_briefings(self, max_pages: int = 3) -> List[Dict]:
        """
        H2HUBì—ì„œ ë¸Œë¦¬í•‘ PDFë¥¼ ìˆ˜ì§‘í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ
        
        Args:
            max_pages: í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            
        Returns:
            List[Dict]: ìˆ˜ì§‘ëœ ë¸Œë¦¬í•‘ ì •ë³´ ë¦¬ìŠ¤íŠ¸
                [
                    {
                        'title': 'ì œëª©',
                        'date': 'ë‚ ì§œ',
                        'pdf_path': 'ë¡œì»¬ PDF ê²½ë¡œ',
                        'url': 'ì›ë³¸ URL'
                    },
                    ...
                ]
        """
        logger.info("=" * 70)
        logger.info("í•œêµ­ìˆ˜ì†Œì—°í•© ë¸Œë¦¬í•‘ ìˆ˜ì§‘ ì‹œì‘")
        logger.info("=" * 70)
        
        collected = []
        
        for page_num in range(1, max_pages + 1):
            logger.info(f"\nğŸ“„ {page_num}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
            
            # í˜ì´ì§€ ì˜¤í”„ì…‹ ê³„ì‚° (10ê°œì”©)
            offset = (page_num - 1) * 10
            
            # ê²Œì‹œíŒ HTML ê°€ì ¸ì˜¤ê¸°
            articles = self._fetch_article_list(offset)
            
            if not articles:
                logger.warning(f"{page_num}í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            logger.info(f"  âœ {len(articles)}ê°œì˜ ê²Œì‹œê¸€ ë°œê²¬")
            
            # ê° ê²Œì‹œê¸€ ì²˜ë¦¬
            for article in articles:
                result = self._process_article(article)
                
                if result:
                    collected.append(result)
                    time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
        
        logger.info("\n" + "=" * 70)
        logger.info(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(collected)}ê°œì˜ ë¸Œë¦¬í•‘ ë‹¤ìš´ë¡œë“œ")
        logger.info("=" * 70)
        
        return collected
    
    def _fetch_article_list(self, offset: int = 0) -> List[Dict]:
        """
        ê²Œì‹œíŒ ëª©ë¡ í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
        
        Args:
            offset: í˜ì´ì§€ ì˜¤í”„ì…‹ (0, 10, 20, ...)
            
        Returns:
            List[Dict]: ê²Œì‹œê¸€ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # URL ìƒì„±
            url = f"{config.H2HUB_PERIODICALS_URL}?mode=list&article.offset={offset}&articleLimit=10"
            
            logger.debug(f"  ìš”ì²­ URL: {url}")
            
            # HTTP ìš”ì²­
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ê²Œì‹œê¸€ ëª©ë¡ ì°¾ê¸° (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
            articles = []
            
            # td.b-td-left ì•ˆì˜ div.b-title-box ì°¾ê¸°
            for td in soup.find_all('td', class_='b-td-left'):
                title_box = td.find('div', class_='b-title-box')
                
                if not title_box:
                    continue
                
                # ë§í¬ì™€ ì œëª© ì¶”ì¶œ
                link = title_box.find('a')
                title_span = title_box.find('span', class_='b-title')
                date_span = title_box.find('span', class_='b-date')
                
                if not (link and title_span):
                    continue
                
                title = title_span.get_text(strip=True)
                href = link.get('href', '')
                date = date_span.get_text(strip=True) if date_span else ''
                
                # "ë¸Œë¦¬í•‘" í‚¤ì›Œë“œ í•„í„°ë§
                if not any(keyword in title for keyword in config.BRIEFING_KEYWORDS):
                    continue
                
                # ìƒì„¸ URL ìƒì„±
                detail_url = urljoin(config.H2HUB_BASE_URL, href)
                
                articles.append({
                    'title': title,
                    'date': date,
                    'detail_url': detail_url
                })
                
                logger.debug(f"    - {title} ({date})")
            
            return articles
            
        except requests.exceptions.RequestException as e:
            logger.error(f"  âŒ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return []
        
        except Exception as e:
            logger.error(f"  âŒ íŒŒì‹± ì˜¤ë¥˜: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return []
    
    def _process_article(self, article: Dict) -> Optional[Dict]:
        """
        ê°œë³„ ê²Œì‹œê¸€ ì²˜ë¦¬ (PDF ë‹¤ìš´ë¡œë“œ)
        
        Args:
            article: ê²Œì‹œê¸€ ì •ë³´
            
        Returns:
            Dict: ì²˜ë¦¬ ê²°ê³¼ ë˜ëŠ” None
        """
        title = article['title']
        date = article['date']
        detail_url = article['detail_url']
        
        logger.info(f"\n  ğŸ“ ì²˜ë¦¬ ì¤‘: {title}")
        
        try:
            # ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼
            response = self.session.get(detail_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # PDF ë§í¬ ì°¾ê¸°
            pdf_url = self._find_pdf_link(soup)
            
            if not pdf_url:
                logger.warning("    âš ï¸ PDF ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            # PDF ë‹¤ìš´ë¡œë“œ
            pdf_path = self._download_pdf(pdf_url, title, date)
            
            if not pdf_path:
                return None
            
            logger.info(f"    âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {Path(pdf_path).name}")
            
            return {
                'title': title,
                'date': date,
                'pdf_path': pdf_path,
                'url': detail_url
            }
            
        except Exception as e:
            logger.error(f"    âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _find_pdf_link(self, soup: BeautifulSoup) -> Optional[str]:
        """
        ìƒì„¸ í˜ì´ì§€ì—ì„œ PDF ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸°
        
        Args:
            soup: BeautifulSoup ê°ì²´
            
        Returns:
            str: PDF URL ë˜ëŠ” None
        """
        # ë°©ë²• 1: .hwp ë˜ëŠ” .pdf í™•ì¥ìê°€ ìˆëŠ” ë§í¬ ì°¾ê¸°
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            
            if href.endswith('.pdf') or '.pdf' in href.lower():
                return urljoin(config.H2HUB_BASE_URL, href)
        
        # ë°©ë²• 2: "ë°”ë¡œë³´ê¸°" ë˜ëŠ” "ë‹¤ìš´ë¡œë“œ" ë²„íŠ¼ ì°¾ê¸°
        for link in soup.find_all('a'):
            link_text = link.get_text(strip=True)
            
            if any(keyword in link_text for keyword in ['ë°”ë¡œë³´ê¸°', 'ë‹¤ìš´ë¡œë“œ', 'PDF', 'pdf']):
                href = link.get('href', '')
                if href:
                    return urljoin(config.H2HUB_BASE_URL, href)
        
        # ë°©ë²• 3: input[type="hidden"] ì—ì„œ íŒŒì¼ ì •ë³´ ì°¾ê¸°
        file_input = soup.find('input', {'type': 'hidden', 'name': re.compile(r'file|attach', re.I)})
        if file_input and file_input.get('value'):
            file_value = file_input['value']
            if file_value.endswith('.pdf'):
                return urljoin(config.H2HUB_BASE_URL, file_value)
        
        return None
    
    def _download_pdf(self, pdf_url: str, title: str, date: str) -> Optional[str]:
        """
        PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        
        Args:
            pdf_url: PDF URL
            title: ì œëª©
            date: ë‚ ì§œ
            
        Returns:
            str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
        """
        try:
            # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
            safe_title = re.sub(r'[^\w\s-]', '', title)
            safe_title = re.sub(r'[-\s]+', '_', safe_title)
            
            # ë‚ ì§œ í¬ë§·íŒ… (YYYY-MM-DD â†’ YYMMDD)
            if date and len(date) >= 10:
                date_str = date.replace('-', '').replace('.', '')[2:8]  # 25.12.03 â†’ 251203
            else:
                date_str = time.strftime('%y%m%d')
            
            filename = f"{date_str}_{safe_title}.pdf"
            filepath = self.download_dir / filename
            
            # ì´ë¯¸ ë‹¤ìš´ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ ìŠ¤í‚µ
            if filepath.exists():
                logger.info(f"    â„¹ï¸ ì´ë¯¸ ì¡´ì¬: {filename}")
                return str(filepath)
            
            # PDF ë‹¤ìš´ë¡œë“œ
            logger.debug(f"    ë‹¤ìš´ë¡œë“œ URL: {pdf_url}")
            response = self.session.get(pdf_url, timeout=30, stream=True)
            response.raise_for_status()
            
            # íŒŒì¼ ì €ì¥
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            return str(filepath)
            
        except Exception as e:
            logger.error(f"    âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None


def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    collector = H2HubCollector()
    results = collector.collect_briefings(max_pages=2)
    
    print("\n" + "=" * 70)
    print(f"ìˆ˜ì§‘ ì™„ë£Œ: {len(results)}ê°œ")
    print("=" * 70)
    
    for result in results:
        print(f"\nì œëª©: {result['title']}")
        print(f"ë‚ ì§œ: {result['date']}")
        print(f"íŒŒì¼: {result['pdf_path']}")


if __name__ == "__main__":
    main()