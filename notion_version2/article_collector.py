#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•œêµ­ìˆ˜ì†Œì—°í•©(H2HUB) PDF ë¸Œë¦¬í•‘ ìˆ˜ì§‘ ëª¨ë“ˆ
ê²Œì‹œíŒì—ì„œ "ë¸Œë¦¬í•‘" í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²Œì‹œê¸€ì˜ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
import re
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
import time

import config

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=getattr(logging, config.LOG_LEVEL),
    format=config.LOG_FORMAT
)
logger = logging.getLogger(__name__)


class H2HUBBriefingCollector:
    """
    í•œêµ­ìˆ˜ì†Œì—°í•©(H2HUB) ê²Œì‹œíŒì—ì„œ ë¸Œë¦¬í•‘ PDFë¥¼ ìë™ ìˆ˜ì§‘í•˜ëŠ” í´ë˜ìŠ¤
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. ê²Œì‹œíŒ ëª©ë¡ í˜ì´ì§€ í¬ë¡¤ë§
    2. "ë¸Œë¦¬í•‘" í‚¤ì›Œë“œ í•„í„°ë§
    3. ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ì—ì„œ PDF ë‹¤ìš´ë¡œë“œ
    """
    
    def __init__(self):
        self.base_url = config.H2HUB_BASE_URL
        self.periodicals_url = config.H2HUB_PERIODICALS_URL
        self.headers = config.DEFAULT_HEADERS
        self.downloads_dir = config.DOWNLOADS_DIR
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        logger.info(f"H2HUB Collector ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ë‹¤ìš´ë¡œë“œ ê²½ë¡œ: {self.downloads_dir}")
    
    def collect_briefings(self, max_pages: int = 3) -> List[Dict]:
        """
        ë¸Œë¦¬í•‘ PDFë¥¼ ìˆ˜ì§‘í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ
        
        Args:
            max_pages: ìˆ˜ì§‘í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            
        Returns:
            List[Dict]: ìˆ˜ì§‘ëœ ë¸Œë¦¬í•‘ ì •ë³´ ë¦¬ìŠ¤íŠ¸
                [
                    {
                        'title': 'ì œëª©',
                        'date': 'ë‚ ì§œ',
                        'url': 'ì›ë¬¸ URL',
                        'pdf_path': 'PDF ë¡œì»¬ ê²½ë¡œ'
                    },
                    ...
                ]
        """
        logger.info("=" * 70)
        logger.info("í•œêµ­ìˆ˜ì†Œì—°í•© ë¸Œë¦¬í•‘ ìˆ˜ì§‘ ì‹œì‘")
        logger.info("=" * 70)
        
        all_briefings = []
        
        for page in range(max_pages):
            logger.info(f"\nğŸ“„ {page + 1}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
            
            # ê²Œì‹œíŒ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            articles = self._fetch_article_list(page)
            
            if not articles:
                logger.warning(f"{page + 1}í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            # ë¸Œë¦¬í•‘ í•„í„°ë§ ë° ë‹¤ìš´ë¡œë“œ
            for article in articles:
                if self._is_briefing(article['title']):
                    logger.info(f"âœ… ë¸Œë¦¬í•‘ ë°œê²¬: {article['title']}")
                    
                    # PDF ë‹¤ìš´ë¡œë“œ
                    pdf_info = self._download_pdf(article)
                    
                    if pdf_info:
                        all_briefings.append(pdf_info)
                        time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            
            time.sleep(2)  # í˜ì´ì§€ ê°„ ë”œë ˆì´
        
        logger.info("\n" + "=" * 70)
        logger.info(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_briefings)}ê°œì˜ ë¸Œë¦¬í•‘ ë‹¤ìš´ë¡œë“œ")
        logger.info("=" * 70)
        
        return all_briefings
    
    def _fetch_article_list(self, page: int = 0) -> List[Dict]:
        """
        ê²Œì‹œíŒ ëª©ë¡ í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
        
        Args:
            page: í˜ì´ì§€ ë²ˆí˜¸ (0ë¶€í„° ì‹œì‘)
            
        Returns:
            List[Dict]: ê²Œì‹œê¸€ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # í˜ì´ì§• íŒŒë¼ë¯¸í„° ì¶”ê°€
            params = {
                'article.offset': page * 10,
                'articleLimit': 10
            }
            
            response = self.session.get(
                self.periodicals_url,
                params=params,
                timeout=15
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            articles = []
            
            # ê²Œì‹œíŒ í…Œì´ë¸”ì—ì„œ ê²Œì‹œê¸€ ì¶”ì¶œ
            # (ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ê²Œ ì„ íƒì ì¡°ì • í•„ìš”)
            article_rows = soup.select('table.board-list tbody tr')
            
            if not article_rows:
                # ë‹¤ë¥¸ êµ¬ì¡° ì‹œë„
                article_rows = soup.select('div.board-list li')
            
            for row in article_rows:
                try:
                    # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ (êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                    title_elem = row.select_one('a.title, td.title a, div.title a')
                    
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    link = title_elem.get('href', '')
                    
                    if not link:
                        continue
                    
                    # ì ˆëŒ€ URL ìƒì„±
                    full_url = urljoin(self.base_url, link)
                    
                    # ë‚ ì§œ ì¶”ì¶œ (ìˆëŠ” ê²½ìš°)
                    date_elem = row.select_one('td.date, span.date, div.date')
                    date = date_elem.get_text(strip=True) if date_elem else None
                    
                    articles.append({
                        'title': title,
                        'url': full_url,
                        'date': date
                    })
                    
                except Exception as e:
                    logger.debug(f"ê²Œì‹œê¸€ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            logger.info(f"  âœ {len(articles)}ê°œì˜ ê²Œì‹œê¸€ ë°œê²¬")
            return articles
            
        except Exception as e:
            logger.error(f"ê²Œì‹œíŒ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return []
    
    def _is_briefing(self, title: str) -> bool:
        """
        ì œëª©ì— ë¸Œë¦¬í•‘ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        
        Args:
            title: ê²Œì‹œê¸€ ì œëª©
            
        Returns:
            bool: ë¸Œë¦¬í•‘ ì—¬ë¶€
        """
        for keyword in config.BRIEFING_KEYWORDS:
            if keyword in title:
                return True
        return False
    
    def _download_pdf(self, article: Dict) -> Optional[Dict]:
        """
        ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ì—ì„œ PDF ë‹¤ìš´ë¡œë“œ
        
        Args:
            article: ê²Œì‹œê¸€ ì •ë³´
            
        Returns:
            Dict: ë‹¤ìš´ë¡œë“œëœ PDF ì •ë³´ (ì‹¤íŒ¨ ì‹œ None)
        """
        try:
            # ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼
            response = self.session.get(article['url'], timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # PDF ì²¨ë¶€íŒŒì¼ ë§í¬ ì°¾ê¸°
            pdf_link = self._find_pdf_link(soup)
            
            if not pdf_link:
                logger.warning(f"  âš ï¸ PDF ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {article['title']}")
                return None
            
            # PDF ë‹¤ìš´ë¡œë“œ
            pdf_path = self._download_file(pdf_link, article['title'])
            
            if not pdf_path:
                return None
            
            # ë‚ ì§œ íŒŒì‹±
            date = self._parse_date(article.get('date'), soup)
            
            return {
                'title': article['title'],
                'date': date,
                'url': article['url'],
                'pdf_path': str(pdf_path)
            }
            
        except Exception as e:
            logger.error(f"  âŒ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({article['title']}): {e}")
            return None
    
    def _find_pdf_link(self, soup: BeautifulSoup) -> Optional[str]:
        """
        ìƒì„¸ í˜ì´ì§€ì—ì„œ PDF ì²¨ë¶€íŒŒì¼ ë§í¬ ì°¾ê¸°
        
        Args:
            soup: BeautifulSoup ê°ì²´
            
        Returns:
            str: PDF ë§í¬ (ì—†ìœ¼ë©´ None)
        """
        # ì²¨ë¶€íŒŒì¼ ì˜ì—­ì—ì„œ PDF ë§í¬ ì°¾ê¸°
        # (ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ ì„ íƒì ì¡°ì • í•„ìš”)
        
        # ë°©ë²• 1: ì²¨ë¶€íŒŒì¼ ì˜ì—­ì—ì„œ ì°¾ê¸°
        attach_area = soup.select_one('div.attach, div.file-list, ul.attach-list')
        
        if attach_area:
            pdf_links = attach_area.select('a[href*=".pdf"], a[href*="download"]')
            
            for link in pdf_links:
                href = link.get('href', '')
                if '.pdf' in href.lower() or 'download' in href.lower():
                    return urljoin(self.base_url, href)
        
        # ë°©ë²• 2: ì „ì²´ í˜ì´ì§€ì—ì„œ PDF ë§í¬ ì°¾ê¸°
        all_links = soup.select('a[href*=".pdf"]')
        
        if all_links:
            return urljoin(self.base_url, all_links[0].get('href', ''))
        
        # ë°©ë²• 3: download íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ” ë§í¬ ì°¾ê¸°
        download_links = soup.select('a[href*="download"], a[href*="fileDown"]')
        
        if download_links:
            return urljoin(self.base_url, download_links[0].get('href', ''))
        
        return None
    
    def _download_file(self, url: str, title: str) -> Optional[Path]:
        """
        íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥
        
        Args:
            url: ë‹¤ìš´ë¡œë“œ URL
            title: ê²Œì‹œê¸€ ì œëª© (íŒŒì¼ëª… ìƒì„±ìš©)
            
        Returns:
            Path: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ (ì‹¤íŒ¨ ì‹œ None)
        """
        try:
            response = self.session.get(url, timeout=30, stream=True)
            response.raise_for_status()
            
            # íŒŒì¼ëª… ìƒì„± (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
            safe_title = re.sub(r'[^\w\s-]', '', title)
            safe_title = re.sub(r'[-\s]+', '_', safe_title)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{timestamp}_{safe_title[:50]}.pdf"
            
            filepath = self.downloads_dir / filename
            
            # íŒŒì¼ ì €ì¥
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            file_size = filepath.stat().st_size / 1024  # KB
            logger.info(f"  âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filename} ({file_size:.1f} KB)")
            
            return filepath
            
        except Exception as e:
            logger.error(f"  âŒ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _parse_date(self, date_str: Optional[str], soup: BeautifulSoup) -> str:
        """
        ë‚ ì§œ íŒŒì‹± ë° í¬ë§·íŒ…
        
        Args:
            date_str: ë‚ ì§œ ë¬¸ìì—´
            soup: ìƒì„¸ í˜ì´ì§€ BeautifulSoup (ë‚ ì§œ ì¶”ì¶œìš©)
            
        Returns:
            str: YYYY-MM-DD í˜•ì‹ì˜ ë‚ ì§œ
        """
        # ë‚ ì§œ ë¬¸ìì—´ì´ ìˆìœ¼ë©´ íŒŒì‹± ì‹œë„
        if date_str:
            try:
                # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
                for fmt in ['%Y-%m-%d', '%Y.%m.%d', '%Y/%m/%d', '%Y%m%d']:
                    try:
                        dt = datetime.strptime(date_str.replace('.', '-').replace('/', '-')[:10], fmt)
                        return dt.strftime('%Y-%m-%d')
                    except:
                        continue
            except:
                pass
        
        # ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚ ì§œ ì°¾ê¸°
        if soup:
            date_elem = soup.select_one('span.date, td.date, div.date, p.date')
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                # ìˆ«ìë§Œ ì¶”ì¶œ (YYYYMMDD í˜•ì‹)
                date_numbers = re.findall(r'\d+', date_text)
                if len(date_numbers) >= 3:
                    try:
                        year = int(date_numbers[0])
                        month = int(date_numbers[1])
                        day = int(date_numbers[2])
                        return f"{year:04d}-{month:02d}-{day:02d}"
                    except:
                        pass
        
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì˜¤ëŠ˜ ë‚ ì§œ ë°˜í™˜
        return datetime.now().strftime('%Y-%m-%d')


def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    collector = H2HUBBriefingCollector()
    briefings = collector.collect_briefings(max_pages=2)
    
    print("\n" + "=" * 70)
    print("ìˆ˜ì§‘ ê²°ê³¼:")
    print("=" * 70)
    
    for i, briefing in enumerate(briefings, 1):
        print(f"\n{i}. {briefing['title']}")
        print(f"   ë‚ ì§œ: {briefing['date']}")
        print(f"   URL: {briefing['url']}")
        print(f"   PDF: {briefing['pdf_path']}")


if __name__ == "__main__":
    main()
