#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""í•œêµ­ìˆ˜ì†Œì—°í•©(H2HUB) PDF ë¸Œë¦¬í•‘ ìˆ˜ì§‘ ëª¨ë“ˆ"""

import logging
import time
import re
from pathlib import Path
from typing import List, Dict, Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

import config

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class H2HUBBriefingCollector:
    """H2HUB ë¸Œë¦¬í•‘ ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(config.DEFAULT_HEADERS)
        self.download_dir = config.DOWNLOADS_DIR
        logger.info("H2HUB Collector ì´ˆê¸°í™” ì™„ë£Œ")
    
    def collect_briefings(self, max_pages: int = 3) -> List[Dict]:
        """ë¸Œë¦¬í•‘ PDF ìˆ˜ì§‘"""
        logger.info("=" * 70)
        logger.info("í•œêµ­ìˆ˜ì†Œì—°í•© ë¸Œë¦¬í•‘ ìˆ˜ì§‘ ì‹œì‘")
        logger.info("=" * 70)
        
        collected = []
        
        for page_num in range(1, max_pages + 1):
            logger.info(f"\nğŸ“„ {page_num}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
            
            offset = (page_num - 1) * 10
            articles = self._fetch_article_list(offset)
            
            if not articles:
                logger.warning(f"{page_num}í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ ì—†ìŒ")
                break
            
            logger.info(f"  âœ {len(articles)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
            
            for article in articles:
                result = self._process_article(article)
                if result:
                    collected.append(result)
                    time.sleep(1)
        
        logger.info(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(collected)}ê°œ")
        return collected
    
    def _fetch_article_list(self, offset: int = 0) -> List[Dict]:
        """ê²Œì‹œíŒ ëª©ë¡ í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ"""
        try:
            url = f"{config.H2HUB_PERIODICALS_URL}?mode=list&article.offset={offset}&articleLimit=10"
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            articles = []
            
            for td in soup.find_all('td', class_='b-td-left'):
                title_box = td.find('div', class_='b-title-box')
                if not title_box:
                    continue
                
                link = title_box.find('a')
                title_span = title_box.find('span', class_='b-title')
                date_span = title_box.find('span', class_='b-date')
                
                if not (link and title_span):
                    continue
                
                title = title_span.get_text(strip=True)
                href = link.get('href', '')
                date = date_span.get_text(strip=True) if date_span else ''
                
                # "ë¸Œë¦¬í•‘" í‚¤ì›Œë“œ í•„í„°ë§
                if not any(keyword in title for keyword in config.BRIEFING_KEYWORDS):
                    continue
                
                detail_url = urljoin(config.H2HUB_BASE_URL, href)
                articles.append({
                    'title': title,
                    'date': date,
                    'detail_url': detail_url
                })
            
            return articles
        except Exception as e:
            logger.error(f"  âŒ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return []
    
    def _process_article(self, article: Dict) -> Optional[Dict]:
        """ê°œë³„ ê²Œì‹œê¸€ ì²˜ë¦¬ (PDF ë‹¤ìš´ë¡œë“œ)"""
        title = article['title']
        date = article['date']
        detail_url = article['detail_url']
        
        logger.info(f"\n  ğŸ“ ì²˜ë¦¬ ì¤‘: {title}")
        
        try:
            response = self.session.get(detail_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            pdf_url = self._find_pdf_link(soup)
            
            if not pdf_url:
                logger.warning("    âš ï¸ PDF ë§í¬ ì—†ìŒ")
                return None
            
            pdf_path = self._download_pdf(pdf_url, title, date)
            
            if not pdf_path:
                return None
            
            logger.info(f"    âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {Path(pdf_path).name}")
            
            return {
                'title': title,
                'date': date,
                'pdf_path': pdf_path,
                'url': detail_url
            }
        except Exception as e:
            logger.error(f"    âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _find_pdf_link(self, soup: BeautifulSoup) -> Optional[str]:
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ PDF ë§í¬ ì°¾ê¸°"""
        # .pdf í™•ì¥ì ë§í¬ ì°¾ê¸°
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            if href.endswith('.pdf') or '.pdf' in href.lower():
                return urljoin(config.H2HUB_BASE_URL, href)
        
        # "ë°”ë¡œë³´ê¸°" ë²„íŠ¼ ì°¾ê¸°
        for link in soup.find_all('a'):
            link_text = link.get_text(strip=True)
            if any(keyword in link_text for keyword in ['ë°”ë¡œë³´ê¸°', 'ë‹¤ìš´ë¡œë“œ', 'PDF']):
                href = link.get('href', '')
                if href:
                    return urljoin(config.H2HUB_BASE_URL, href)
        
        return None
    
    def _download_pdf(self, pdf_url: str, title: str, date: str) -> Optional[str]:
        """PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
            safe_title = re.sub(r'[^\w\s-]', '', title)
            safe_title = re.sub(r'[-\s]+', '_', safe_title)
            
            # ë‚ ì§œ í¬ë§·íŒ…
            if date and len(date) >= 10:
                date_str = date.replace('-', '').replace('.', '')[2:8]
            else:
                date_str = time.strftime('%y%m%d')
            
            filename = f"{date_str}_{safe_title}.pdf"
            filepath = self.download_dir / filename
            
            # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
            if filepath.exists():
                logger.info(f"    â„¹ï¸ ì´ë¯¸ ì¡´ì¬: {filename}")
                return str(filepath)
            
            # PDF ë‹¤ìš´ë¡œë“œ
            response = self.session.get(pdf_url, timeout=30, stream=True)
            response.raise_for_status()
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            return str(filepath)
        except Exception as e:
            logger.error(f"    âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None


def main():
    """í…ŒìŠ¤íŠ¸ìš©"""
    collector = H2HUBBriefingCollector()
    results = collector.collect_briefings(max_pages=2)
    
    print(f"\nìˆ˜ì§‘ ì™„ë£Œ: {len(results)}ê°œ")
    for result in results:
        print(f"\nì œëª©: {result['title']}")
        print(f"íŒŒì¼: {result['pdf_path']}")


if __name__ == "__main__":
    main()