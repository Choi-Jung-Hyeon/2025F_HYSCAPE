# source_fetcher/google_fetcher.py
"""
êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ Fetcher
- í‚¤ì›Œë“œ ê¸°ë°˜ êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰
- HTML íŒŒì‹± ë°©ì‹ (API í‚¤ ë¶ˆí•„ìš”)
"""

import requests
from bs4 import BeautifulSoup
from typing import List, Dict
from urllib.parse import quote
from .api_fetcher import APIFetcher

class GoogleFetcher(APIFetcher):
    """
    êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ (HTML íŒŒì‹± ë°©ì‹)
    
    ì‚¬ìš© ì˜ˆ:
        from config import GOOGLE_KEYWORDS, MAX_GOOGLE_PER_KEYWORD
        fetcher = GoogleFetcher()
        articles = fetcher.fetch_articles_by_keywords(GOOGLE_KEYWORDS, MAX_GOOGLE_PER_KEYWORD)
    """
    
    def __init__(self, **kwargs):
        """
        êµ¬ê¸€ ë‰´ìŠ¤ Fetcher ì´ˆê¸°í™”
        """
        super().__init__(source_name="êµ¬ê¸€ë‰´ìŠ¤", **kwargs)
        self.base_url = "https://www.google.com/search"
    
    def fetch_articles_by_keywords(self, keywords: List[str], max_per_keyword: int = 3) -> List[Dict]:
        """
        í‚¤ì›Œë“œ ëª©ë¡ìœ¼ë¡œ êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰
        
        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡
            max_per_keyword: í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
            
        Returns:
            List[Dict]: ê¸°ì‚¬ ëª©ë¡
        """
        all_articles = []
        
        for keyword in keywords:
            try:
                self.logger.info(f"ğŸ” êµ¬ê¸€ ê²€ìƒ‰: '{keyword}'")
                
                # ê²€ìƒ‰ íŒŒë¼ë¯¸í„° (ë‰´ìŠ¤ íƒ­)
                params = {
                    'q': keyword,
                    'tbm': 'nws',  # ë‰´ìŠ¤ íƒ­
                    'hl': 'ko',     # í•œêµ­ì–´
                    'gl': 'kr',     # í•œêµ­ ì§€ì—­
                    'num': max_per_keyword  # ê²°ê³¼ ê°œìˆ˜
                }
                
                # í—¤ë” ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Referer': 'https://www.google.com/'
                }
                
                response = requests.get(self.base_url, params=params, headers=headers, timeout=10)
                response.raise_for_status()
                
                # HTML íŒŒì‹±
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # êµ¬ê¸€ ë‰´ìŠ¤ ê²°ê³¼ ì¶”ì¶œ (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
                news_items = []
                
                # ì„ íƒì 1: ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ê²°ê³¼
                news_items = soup.select('div.SoaBEf')[:max_per_keyword]
                
                # ì„ íƒì 2: ëŒ€ì²´ ì„ íƒì
                if not news_items:
                    news_items = soup.select('div[data-sokoban-container]')[:max_per_keyword]
                
                # ì„ íƒì 3: ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼
                if not news_items:
                    news_items = soup.select('div.g')[:max_per_keyword]
                
                if not news_items:
                    self.logger.warning(f"âš ï¸ '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    continue
                
                # ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
                for item in news_items:
                    try:
                        # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
                        title_elem = item.select_one('div[role="heading"]') or \
                                   item.select_one('h3') or \
                                   item.select_one('.n0jPhd')
                        
                        link_elem = item.select_one('a')
                        
                        if not title_elem or not link_elem:
                            continue
                        
                        title = title_elem.get_text(strip=True)
                        url = link_elem.get('href', '')
                        
                        # êµ¬ê¸€ ë¦¬ë‹¤ì´ë ‰íŠ¸ URL ì •ë¦¬
                        if url.startswith('/url?q='):
                            url = url.split('/url?q=')[1].split('&')[0]
                        
                        # ë°œí–‰ì¼ì‹œ ì¶”ì¶œ
                        date_elem = item.select_one('.OSrXXb') or \
                                  item.select_one('.LfVVr') or \
                                  item.select_one('span.f')
                        published = date_elem.get_text(strip=True) if date_elem else ''
                        
                        # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
                        source_elem = item.select_one('.CEMjEf') or \
                                    item.select_one('.NUnG9d span')
                        press = source_elem.get_text(strip=True) if source_elem else ''
                        
                        article = {
                            'title': title,
                            'url': url,
                            'published': published,
                            'source': f"{self.source_name}({keyword})",
                            'press': press,
                            'keyword': keyword
                        }
                        
                        if self.validate_article(article) and not url.startswith('/search'):
                            all_articles.append(article)
                            
                    except Exception as e:
                        self.logger.debug(f"ê¸°ì‚¬ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
                
                self.logger.info(f"  âœ… '{keyword}': {len([a for a in all_articles if a.get('keyword') == keyword])}ê°œ")
                
            except Exception as e:
                self.logger.error(f"âŒ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                self.log_error(e)
                continue
        
        self.log_success(len(all_articles))
        return all_articles


# ========================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ========================================
if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 70)
    print("ğŸ§ª GoogleFetcher í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # í…ŒìŠ¤íŠ¸ í‚¤ì›Œë“œ
    test_keywords = ["hydrogen fuel cell", "water electrolysis", "green hydrogen"]
    
    fetcher = GoogleFetcher()
    articles = fetcher.fetch_articles_by_keywords(test_keywords, max_per_keyword=2)
    
    print(f"\nâœ… ì´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
    for i, article in enumerate(articles, 1):
        print(f"{i}. [{article['keyword']}] {article['title'][:60]}...")
        print(f"   ì–¸ë¡ ì‚¬: {article.get('press', 'N/A')}")
        print(f"   URL: {article['url']}")
        print()