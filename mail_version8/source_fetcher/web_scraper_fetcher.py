#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì›¹ í¬ë¡¤ë§ ê¸°ë°˜ ë‰´ìŠ¤ ìˆ˜ì§‘ (ìˆ˜ì • ë²„ì „ v7.1)
- ì´ˆê¸°í™” ë©”ì„œë“œ ìˆ˜ì •: ë‹¤ë¥¸ Fetcherì™€ ì¼ê´€ëœ ì¸í„°í˜ì´ìŠ¤
- User-Agent í—¤ë”ë¡œ ë´‡ ì°¨ë‹¨ ìš°íšŒ
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict, Optional

from source_fetcher.base_fetcher import BaseSourceFetcher

class WebScraperFetcher(BaseSourceFetcher):
    """
    ì›¹ í¬ë¡¤ë§ ê¸°ë°˜ ë‰´ìŠ¤ ìˆ˜ì§‘
    HTML í˜ì´ì§€ì—ì„œ CSS ì…€ë ‰í„°ë¥¼ ì´ìš©í•´ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
    
    ì‚¬ìš© ì˜ˆ:
        fetcher = WebScraperFetcher(
            source_name="H2 View",
            url="https://www.h2-view.com/news/all-news/",
            article_selector="article.post",
            title_selector="h2.entry-title",
            link_selector="a"
        )
        articles = fetcher.fetch_articles(max_articles=5)
    """
    
    def __init__(self, source_name: str, url: str, 
                 article_selector: str, title_selector: str, 
                 link_selector: str, date_selector: str = None, **kwargs):
        """
        ì´ˆê¸°í™” (ìˆ˜ì •ë¨ - ê°œë³„ íŒŒë¼ë¯¸í„° ë°©ì‹)
        
        Args:
            source_name: ì†ŒìŠ¤ ì´ë¦„
            url: í¬ë¡¤ë§í•  URL
            article_selector: ê¸°ì‚¬ ì»¨í…Œì´ë„ˆ CSS ì„ íƒì
            title_selector: ì œëª© CSS ì„ íƒì
            link_selector: ë§í¬ CSS ì„ íƒì
            date_selector: ë‚ ì§œ CSS ì„ íƒì (ì„ íƒ)
            **kwargs: ì¶”ê°€ ì„¤ì • (timeout, headers ë“±)
        """
        super().__init__(source_name, url, **kwargs)
        
        self.article_selector = article_selector
        self.title_selector = title_selector
        self.link_selector = link_selector
        self.date_selector = date_selector
        
        # User-Agent í—¤ë” ì¶”ê°€ (ë´‡ ì°¨ë‹¨ ìš°íšŒ) â­
        self.headers = kwargs.get('headers', {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,ko;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        self.timeout = kwargs.get('timeout', 10)
    
    def fetch_articles(self, max_articles: int = 10) -> List[Dict]:
        """
        ì›¹ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘
        
        Args:
            max_articles: ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
            
        Returns:
            List[Dict]: ê¸°ì‚¬ ëª©ë¡
        """
        self.logger.info(f"ì›¹ í¬ë¡¤ë§ ì‹œì‘: {self.url}")
        articles = []
        
        try:
            # HTTP ìš”ì²­ (User-Agent í¬í•¨)
            response = requests.get(
                self.url, 
                headers=self.headers,
                timeout=self.timeout
            )
            response.raise_for_status()
            
            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ê¸°ì‚¬ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            article_elements = soup.select(self.article_selector)
            
            if not article_elements:
                self.logger.warning(f"âš ï¸ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (selector: {self.article_selector})")
                return []
            
            # ê° ê¸°ì‚¬ íŒŒì‹±
            for article_elem in article_elements[:max_articles]:
                try:
                    article = self._parse_article(article_elem)
                    if article and self.validate_article(article):
                        articles.append(article)
                except Exception as e:
                    self.logger.debug(f"âš ï¸ ê¸°ì‚¬ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue
            
            self.log_success(len(articles))
            
        except requests.exceptions.HTTPError as e:
            self.logger.error(f"âŒ HTTP ì—ëŸ¬: {e}")
            self.log_error(e)
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬: {e}")
            self.log_error(e)
        except Exception as e:
            self.logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")
            self.log_error(e)
        
        return articles
    
    def _parse_article(self, article_elem) -> Optional[Dict]:
        """
        ê°œë³„ ê¸°ì‚¬ ìš”ì†Œ íŒŒì‹±
        
        Args:
            article_elem: BeautifulSoup ê¸°ì‚¬ ìš”ì†Œ
            
        Returns:
            Dict: ê¸°ì‚¬ ì •ë³´ ë˜ëŠ” None
        """
        # ì œëª© ì¶”ì¶œ
        title_elem = article_elem.select_one(self.title_selector)
        if not title_elem:
            return None
        title = title_elem.get_text(strip=True)
        
        # ë§í¬ ì¶”ì¶œ
        link_elem = article_elem.select_one(self.link_selector)
        if not link_elem:
            return None
        
        url = link_elem.get('href')
        if not url:
            return None
        
        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if url.startswith('/'):
            base_url = '/'.join(self.url.split('/')[:3])
            url = base_url + url
        elif not url.startswith('http'):
            url = self.url.rstrip('/') + '/' + url.lstrip('/')
        
        # ë‚ ì§œ ì¶”ì¶œ (ì„ íƒì‚¬í•­)
        published = None
        if self.date_selector:
            date_elem = article_elem.select_one(self.date_selector)
            if date_elem:
                published = date_elem.get_text(strip=True)
        
        return {
            'source': self.source_name,
            'title': title,
            'url': url,
            'published': published or ''
        }


# ========================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ========================================
if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 70)
    print("ğŸ§ª WebScraperFetcher v7.1 í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # H2 View í…ŒìŠ¤íŠ¸
    print("\n[í…ŒìŠ¤íŠ¸ 1] H2 View í¬ë¡¤ë§")
    fetcher = WebScraperFetcher(
        source_name="H2 View",
        url="https://www.h2-view.com/news/all-news/",
        article_selector="article.post",
        title_selector="h2.entry-title",
        link_selector="a"
    )
    
    articles = fetcher.fetch_articles(max_articles=3)
    
    if articles:
        print(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì„±ê³µ\n")
        for i, article in enumerate(articles, 1):
            print(f"{i}. {article['title'][:60]}...")
            print(f"   URL: {article['url']}\n")
    else:
        print("âš ï¸ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨")