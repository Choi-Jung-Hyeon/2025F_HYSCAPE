# source_fetcher/naver_fetcher.py
"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ Fetcher
- í‚¤ì›Œë“œ ê¸°ë°˜ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
- HTML íŒŒì‹± ë°©ì‹ (API í‚¤ ë¶ˆí•„ìš”)
"""

import requests
from bs4 import BeautifulSoup
from typing import List, Dict
from urllib.parse import quote
from .api_fetcher import APIFetcher

class NaverFetcher(APIFetcher):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (HTML íŒŒì‹± ë°©ì‹)
    
    ì‚¬ìš© ì˜ˆ:
        from config import NAVER_KEYWORDS, MAX_NAVER_PER_KEYWORD
        fetcher = NaverFetcher()
        articles = fetcher.fetch_articles_by_keywords(NAVER_KEYWORDS, MAX_NAVER_PER_KEYWORD)
    """
    
    def __init__(self, **kwargs):
        """
        ë„¤ì´ë²„ ë‰´ìŠ¤ Fetcher ì´ˆê¸°í™”
        """
        super().__init__(source_name="ë„¤ì´ë²„ë‰´ìŠ¤", **kwargs)
        self.base_url = "https://search.naver.com/search.naver"
    
    def fetch_articles_by_keywords(self, keywords: List[str], max_per_keyword: int = 3) -> List[Dict]:
        """
        í‚¤ì›Œë“œ ëª©ë¡ìœ¼ë¡œ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
        
        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡
            max_per_keyword: í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
            
        Returns:
            List[Dict]: ê¸°ì‚¬ ëª©ë¡
        """
        all_articles = []
        
        for keyword in keywords:
            try:
                self.logger.info(f"ğŸ” ë„¤ì´ë²„ ê²€ìƒ‰: '{keyword}'")
                
                # ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
                params = {
                    'where': 'news',
                    'query': keyword,
                    'sort': '1',  # ìµœì‹ ìˆœ
                    'start': 1
                }
                
                # í—¤ë” ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
                }
                
                response = requests.get(self.base_url, params=params, headers=headers, timeout=10)
                response.raise_for_status()
                
                # HTML íŒŒì‹±
                soup = BeautifulSoup(response.text, 'html.parser')
                news_items = soup.select('.news_area')[:max_per_keyword]
                
                if not news_items:
                    self.logger.warning(f"âš ï¸ '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    continue
                
                # ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
                for item in news_items:
                    try:
                        # ì œëª©ê³¼ ë§í¬
                        title_elem = item.select_one('.news_tit')
                        if not title_elem:
                            continue
                        
                        title = title_elem.get_text(strip=True)
                        url = title_elem.get('href', '')
                        
                        # ì–¸ë¡ ì‚¬
                        press_elem = item.select_one('.info.press')
                        press = press_elem.get_text(strip=True) if press_elem else ''
                        
                        # ë°œí–‰ì¼ì‹œ
                        date_elem = item.select_one('.info_group .info')
                        published = date_elem.get_text(strip=True) if date_elem else ''
                        
                        article = {
                            'title': title,
                            'url': url,
                            'published': published,
                            'source': f"{self.source_name}({keyword})",
                            'press': press,
                            'keyword': keyword
                        }
                        
                        if self.validate_article(article):
                            all_articles.append(article)
                            
                    except Exception as e:
                        self.logger.debug(f"ê¸°ì‚¬ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
                
                self.logger.info(f"  âœ… '{keyword}': {len([a for a in all_articles if a.get('keyword') == keyword])}ê°œ")
                
            except Exception as e:
                self.logger.error(f"âŒ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                self.log_error(e)
                continue
        
        self.log_success(len(all_articles))
        return all_articles


# ========================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ========================================
if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 70)
    print("ğŸ§ª NaverFetcher í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # í…ŒìŠ¤íŠ¸ í‚¤ì›Œë“œ
    test_keywords = ["ìˆ˜ì†Œ", "ìˆ˜ì „í•´", "ì—°ë£Œì „ì§€"]
    
    fetcher = NaverFetcher()
    articles = fetcher.fetch_articles_by_keywords(test_keywords, max_per_keyword=2)
    
    print(f"\nâœ… ì´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
    for i, article in enumerate(articles, 1):
        print(f"{i}. [{article['keyword']}] {article['title'][:60]}...")
        print(f"   ì–¸ë¡ ì‚¬: {article.get('press', 'N/A')}")
        print(f"   URL: {article['url']}")
        print()