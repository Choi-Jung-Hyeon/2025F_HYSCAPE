# source_fetcher.py (v2.0)
"""
ë‹¤ì¤‘ ë‰´ìŠ¤ ì†ŒìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ê¸°ì‚¬ URLì„ ìˆ˜ì§‘í•˜ëŠ” ëª¨ë“ˆ
"""

from abc import ABC, abstractmethod
from typing import List, Dict
import feedparser
import requests
from bs4 import BeautifulSoup


class NewsFetcher(ABC):
    """
    ëª¨ë“  ë‰´ìŠ¤ ì†ŒìŠ¤ Fetcherì˜ ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤
    ìƒˆë¡œìš´ ë‰´ìŠ¤ ì†ŒìŠ¤ë¥¼ ì¶”ê°€í•  ë•Œ ì´ í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ êµ¬í˜„í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, source_name: str):
        self.source_name = source_name
    
    @abstractmethod
    def fetch_articles(self) -> List[Dict[str, str]]:
        """
        ë‰´ìŠ¤ ì†ŒìŠ¤ë¡œë¶€í„° ê¸°ì‚¬ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë©”ì„œë“œ
        
        Returns:
            List[Dict]: ê° ê¸°ì‚¬ëŠ” {'title': str, 'url': str, 'source': str} í˜•ì‹
        """
        pass


class RSSFetcher(NewsFetcher):
    """
    RSS í”¼ë“œë¥¼ í†µí•´ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” Fetcher
    """
    
    def __init__(self, source_name: str, rss_url: str):
        super().__init__(source_name)
        self.rss_url = rss_url
    
    def fetch_articles(self) -> List[Dict[str, str]]:
        """RSS í”¼ë“œë¥¼ íŒŒì‹±í•˜ì—¬ ê¸°ì‚¬ ëª©ë¡ì„ ë°˜í™˜"""
        print(f"[{self.source_name}] RSS í”¼ë“œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤: {self.rss_url}")
        
        try:
            feed = feedparser.parse(self.rss_url)
            
            if feed.bozo:  # í”¼ë“œ íŒŒì‹± ì˜¤ë¥˜ ì²´í¬
                print(f"  âš ï¸ RSS í”¼ë“œ íŒŒì‹± ì¤‘ ê²½ê³ ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {feed.bozo_exception}")
            
            articles = []
            for entry in feed.entries:
                article = {
                    'title': entry.get('title', 'ì œëª© ì—†ìŒ'),
                    'url': entry.get('link', ''),
                    'source': self.source_name
                }
                articles.append(article)
            
            print(f"  âœ… {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return articles
            
        except Exception as e:
            print(f"  âŒ RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []


class WebPageFetcher(NewsFetcher):
    """
    ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” Fetcher
    RSSë¥¼ ì œê³µí•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸ë¥¼ ìœ„í•œ í´ë˜ìŠ¤
    """
    
    def __init__(self, source_name: str, page_url: str, selectors: Dict[str, str]):
        """
        Args:
            source_name: ë‰´ìŠ¤ ì†ŒìŠ¤ ì´ë¦„
            page_url: í¬ë¡¤ë§í•  ì›¹í˜ì´ì§€ URL
            selectors: CSS ì„ íƒì ë”•ì…”ë„ˆë¦¬
                - 'container': ê¸°ì‚¬ ëª©ë¡ì„ ê°ì‹¸ëŠ” ì»¨í…Œì´ë„ˆ
                - 'title': ê¸°ì‚¬ ì œëª© ì„ íƒì
                - 'link': ê¸°ì‚¬ ë§í¬ ì„ íƒì
        """
        super().__init__(source_name)
        self.page_url = page_url
        self.selectors = selectors
    
    def fetch_articles(self) -> List[Dict[str, str]]:
        """ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ê¸°ì‚¬ ëª©ë¡ì„ ë°˜í™˜"""
        print(f"[{self.source_name}] ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤: {self.page_url}")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(self.page_url, headers=headers, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            articles = []
            
            # ì»¨í…Œì´ë„ˆ ì„ íƒìê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì˜ì—­ ë‚´ì—ì„œë§Œ ê²€ìƒ‰
            if self.selectors.get('container'):
                containers = soup.select(self.selectors['container'])
            else:
                containers = [soup]
            
            for container in containers:
                # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                title_elem = container.select_one(self.selectors.get('title', 'a'))
                link_elem = container.select_one(self.selectors.get('link', 'a'))
                
                if title_elem and link_elem:
                    title = title_elem.get_text(strip=True)
                    url = link_elem.get('href', '')
                    
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if url and not url.startswith('http'):
                        from urllib.parse import urljoin
                        url = urljoin(self.page_url, url)
                    
                    if title and url:
                        article = {
                            'title': title,
                            'url': url,
                            'source': self.source_name
                        }
                        articles.append(article)
            
            print(f"  âœ… {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return articles
            
        except Exception as e:
            print(f"  âŒ ì›¹í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []


class NaverNewsFetcher(NewsFetcher):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ì„ í†µí•´ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” Fetcher
    íŠ¹ì • í‚¤ì›Œë“œë¡œ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤
    """
    
    def __init__(self, keyword: str):
        super().__init__(f"ë„¤ì´ë²„ë‰´ìŠ¤({keyword})")
        self.keyword = keyword
        self.search_url = f"https://search.naver.com/search.naver?where=news&query={keyword}"
    
    def fetch_articles(self) -> List[Dict[str, str]]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ê¸°ì‚¬ ëª©ë¡ì„ ë°˜í™˜"""
        print(f"[{self.source_name}] ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤: {self.search_url}")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(self.search_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            articles = []
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ì„ íƒì
            news_items = soup.select('.news_tit')
            
            for item in news_items[:10]:  # ìµœëŒ€ 10ê°œ
                title = item.get_text(strip=True)
                url = item.get('href', '')
                
                if title and url:
                    article = {
                        'title': title,
                        'url': url,
                        'source': self.source_name
                    }
                    articles.append(article)
            
            print(f"  âœ… {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return articles
            
        except Exception as e:
            print(f"  âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []


class SourceManager:
    """
    ì—¬ëŸ¬ ë‰´ìŠ¤ ì†ŒìŠ¤ë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self):
        self.fetchers: List[NewsFetcher] = []
    
    def add_fetcher(self, fetcher: NewsFetcher):
        """Fetcher ì¶”ê°€"""
        self.fetchers.append(fetcher)
    
    def fetch_all_articles(self, limit_per_source: int = None) -> List[Dict[str, str]]:
        """
        ëª¨ë“  ë“±ë¡ëœ ì†ŒìŠ¤ë¡œë¶€í„° ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘
        
        Args:
            limit_per_source: ì†ŒìŠ¤ë‹¹ ê°€ì ¸ì˜¬ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ (Noneì´ë©´ ì œí•œ ì—†ìŒ)
        
        Returns:
            ëª¨ë“  ì†ŒìŠ¤ì˜ ê¸°ì‚¬ë¥¼ í•©ì¹œ ë¦¬ìŠ¤íŠ¸
        """
        all_articles = []
        
        print("\n" + "="*60)
        print("ë‰´ìŠ¤ ì†ŒìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤")
        print("="*60)
        
        for fetcher in self.fetchers:
            articles = fetcher.fetch_articles()
            
            if limit_per_source and len(articles) > limit_per_source:
                articles = articles[:limit_per_source]
                print(f"  ğŸ“Œ ì†ŒìŠ¤ë‹¹ ì œí•œìœ¼ë¡œ {limit_per_source}ê°œë§Œ ì„ íƒí•©ë‹ˆë‹¤.")
            
            all_articles.extend(articles)
        
        print("\n" + "="*60)
        print(f"ì´ {len(all_articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤")
        print("="*60 + "\n")
        
        return all_articles


# ============================================================
# ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ ë° ì„¤ì •
# ============================================================

def create_fetchers_from_config() -> SourceManager:
    """
    config.pyì˜ NEWS_SOURCESë¥¼ ì½ì–´ì„œ SourceManagerë¥¼ ìƒì„±
    """
    from config import NEWS_SOURCES
    
    manager = SourceManager()
    
    for source_name, rss_url in NEWS_SOURCES.items():
        manager.add_fetcher(
            RSSFetcher(source_name=source_name, rss_url=rss_url)
        )
    
    return manager


# ============================================================
# ëª¨ë“ˆ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
# ============================================================

if __name__ == '__main__':
    print("=" * 60)
    print("source_fetcher.py (v2.0) ë‹¨ìœ„ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # SourceManager ìƒì„± ë° Fetcher ë“±ë¡
    manager = create_fetchers_from_config()
    
    # ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ (ê° ì†ŒìŠ¤ë‹¹ ìµœëŒ€ 3ê°œ)
    articles = manager.fetch_all_articles(limit_per_source=3)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“° ìˆ˜ì§‘ëœ ê¸°ì‚¬ ëª©ë¡:")
    print("-" * 60)
    for i, article in enumerate(articles, 1):
        print(f"\n[{i}] {article['title']}")
        print(f"    ì¶œì²˜: {article['source']}")
        print(f"    ë§í¬: {article['url'][:80]}...")
    
    print("\n" + "=" * 60)
    print("ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 60)