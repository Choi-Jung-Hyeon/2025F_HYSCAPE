# source_fetcher.py (v2.1 - Enhanced)
"""
ë‹¤ì¤‘ ë‰´ìŠ¤ ì†ŒìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ê¸°ì‚¬ URLì„ ìˆ˜ì§‘í•˜ëŠ” ëª¨ë“ˆ
- RSS í”¼ë“œ ìˆ˜ì§‘
- ì›¹í˜ì´ì§€ í¬ë¡¤ë§
- ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
- ì ‘ì† ë¶ˆê°€ëŠ¥í•œ ì†ŒìŠ¤ ì§„ë‹¨
"""

from abc import ABC, abstractmethod
from typing import List, Dict
import feedparser
import requests
from bs4 import BeautifulSoup
import time


class NewsFetcher(ABC):
    """
    ëª¨ë“  ë‰´ìŠ¤ ì†ŒìŠ¤ Fetcherì˜ ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤
    """
    
    def __init__(self, source_name: str):
        self.source_name = source_name
        self.status = "Unknown"  # ìƒíƒœ: Success, Failed, Partial
        self.error_message = None
    
    @abstractmethod
    def fetch_articles(self) -> List[Dict[str, str]]:
        """ë‰´ìŠ¤ ì†ŒìŠ¤ë¡œë¶€í„° ê¸°ì‚¬ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë©”ì„œë“œ"""
        pass


class RSSFetcher(NewsFetcher):
    """RSS í”¼ë“œë¥¼ í†µí•´ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” Fetcher"""
    
    def __init__(self, source_name: str, rss_url: str):
        super().__init__(source_name)
        self.rss_url = rss_url
    
    def fetch_articles(self) -> List[Dict[str, str]]:
        """RSS í”¼ë“œë¥¼ íŒŒì‹±í•˜ì—¬ ê¸°ì‚¬ ëª©ë¡ì„ ë°˜í™˜"""
        print(f"[{self.source_name}] RSS í”¼ë“œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤: {self.rss_url}")
        
        try:
            # User-Agent ì¶”ê°€ë¡œ ì°¨ë‹¨ ë°©ì§€
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            # requestsë¡œ ë¨¼ì € ê°€ì ¸ì˜¨ í›„ feedparserì— ì „ë‹¬
            response = requests.get(self.rss_url, headers=headers, timeout=15)
            response.raise_for_status()
            
            feed = feedparser.parse(response.content)
            
            if feed.bozo:
                print(f"  âš ï¸ RSS í”¼ë“œ íŒŒì‹± ê²½ê³ : {feed.get('bozo_exception', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                self.status = "Partial"
            
            if not feed.entries:
                print(f"  âŒ RSS í”¼ë“œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                self.status = "Failed"
                self.error_message = "Empty feed"
                return []
            
            articles = []
            for entry in feed.entries:
                article = {
                    'title': entry.get('title', 'ì œëª© ì—†ìŒ'),
                    'url': entry.get('link', ''),
                    'source': self.source_name
                }
                if article['url']:  # URLì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                    articles.append(article)
            
            print(f"  âœ… {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            self.status = "Success"
            return articles
            
        except requests.exceptions.Timeout:
            print(f"  âŒ íƒ€ì„ì•„ì›ƒ: ì„œë²„ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤.")
            self.status = "Failed"
            self.error_message = "Timeout"
            return []
            
        except requests.exceptions.ConnectionError:
            print(f"  âŒ ì—°ê²° ì˜¤ë¥˜: ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ë˜ëŠ” ì˜ëª»ëœ URL")
            self.status = "Failed"
            self.error_message = "Connection Error"
            return []
            
        except Exception as e:
            print(f"  âŒ RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.status = "Failed"
            self.error_message = str(e)
            return []


class WebPageFetcher(NewsFetcher):
    """ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” Fetcher"""
    
    def __init__(self, source_name: str, page_url: str, selectors: Dict[str, str]):
        super().__init__(source_name)
        self.page_url = page_url
        self.selectors = selectors
    
    def fetch_articles(self) -> List[Dict[str, str]]:
        """ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ê¸°ì‚¬ ëª©ë¡ì„ ë°˜í™˜"""
        print(f"[{self.source_name}] ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤: {self.page_url}")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(self.page_url, headers=headers, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            articles = []
            
            # ì»¨í…Œì´ë„ˆ ì„ íƒìê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì˜ì—­ ë‚´ì—ì„œë§Œ ê²€ìƒ‰
            if self.selectors.get('container'):
                containers = soup.select(self.selectors['container'])
            else:
                containers = [soup]
            
            for container in containers:
                title_elem = container.select_one(self.selectors.get('title', 'a'))
                link_elem = container.select_one(self.selectors.get('link', 'a'))
                
                if title_elem and link_elem:
                    title = title_elem.get_text(strip=True)
                    url = link_elem.get('href', '')
                    
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if url and not url.startswith('http'):
                        from urllib.parse import urljoin
                        url = urljoin(self.page_url, url)
                    
                    if title and url:
                        article = {
                            'title': title,
                            'url': url,
                            'source': self.source_name
                        }
                        articles.append(article)
            
            print(f"  âœ… {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            self.status = "Success"
            return articles
            
        except Exception as e:
            print(f"  âŒ ì›¹í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.status = "Failed"
            self.error_message = str(e)
            return []


class NaverNewsFetcher(NewsFetcher):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ì„ í†µí•´ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” Fetcher"""
    
    def __init__(self, keyword: str):
        super().__init__(f"ë„¤ì´ë²„ë‰´ìŠ¤({keyword})")
        self.keyword = keyword
        self.search_url = f"https://search.naver.com/search.naver?where=news&query={keyword}&sort=1"  # sort=1: ìµœì‹ ìˆœ
    
    def fetch_articles(self) -> List[Dict[str, str]]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ê¸°ì‚¬ ëª©ë¡ì„ ë°˜í™˜"""
        print(f"[{self.source_name}] ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤: {self.search_url}")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(self.search_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            articles = []
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ì„ íƒì
            news_items = soup.select('.news_tit')
            
            for item in news_items[:10]:  # ìµœëŒ€ 10ê°œ
                title = item.get_text(strip=True)
                url = item.get('href', '')
                
                if title and url:
                    article = {
                        'title': title,
                        'url': url,
                        'source': self.source_name
                    }
                    articles.append(article)
            
            print(f"  âœ… {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            self.status = "Success"
            return articles
            
        except Exception as e:
            print(f"  âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.status = "Failed"
            self.error_message = str(e)
            return []


class SourceManager:
    """ì—¬ëŸ¬ ë‰´ìŠ¤ ì†ŒìŠ¤ë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.fetchers: List[NewsFetcher] = []
    
    def add_fetcher(self, fetcher: NewsFetcher):
        """Fetcher ì¶”ê°€"""
        self.fetchers.append(fetcher)
    
    def fetch_all_articles(self, limit_per_source: int = None) -> List[Dict[str, str]]:
        """
        ëª¨ë“  ë“±ë¡ëœ ì†ŒìŠ¤ë¡œë¶€í„° ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘
        
        Args:
            limit_per_source: ì†ŒìŠ¤ë‹¹ ê°€ì ¸ì˜¬ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
        
        Returns:
            ëª¨ë“  ì†ŒìŠ¤ì˜ ê¸°ì‚¬ë¥¼ í•©ì¹œ ë¦¬ìŠ¤íŠ¸
        """
        all_articles = []
        failed_sources = []
        
        print("\n" + "="*60)
        print("ë‰´ìŠ¤ ì†ŒìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤")
        print("="*60)
        
        for fetcher in self.fetchers:
            articles = fetcher.fetch_articles()
            
            if fetcher.status == "Failed":
                failed_sources.append({
                    'source': fetcher.source_name,
                    'error': fetcher.error_message
                })
            
            if limit_per_source and len(articles) > limit_per_source:
                articles = articles[:limit_per_source]
                print(f"  ğŸ“Œ ì†ŒìŠ¤ë‹¹ ì œí•œìœ¼ë¡œ {limit_per_source}ê°œë§Œ ì„ íƒí•©ë‹ˆë‹¤.")
            
            all_articles.extend(articles)
            time.sleep(1)  # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€
        
        print("\n" + "="*60)
        print(f"ì´ {len(all_articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤")
        
        if failed_sources:
            print("\nâš ï¸ ì ‘ì† ì‹¤íŒ¨í•œ ì†ŒìŠ¤:")
            for failed in failed_sources:
                print(f"  - {failed['source']}: {failed['error']}")
        
        print("="*60 + "\n")
        
        return all_articles


def create_fetchers_from_config() -> SourceManager:
    """
    config.pyì˜ NEWS_SOURCESë¥¼ ì½ì–´ì„œ SourceManagerë¥¼ ìƒì„±
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ë„ í•¨ê»˜ ì¶”ê°€
    """
    from config import NEWS_SOURCES, NAVER_NEWS_KEYWORDS
    
    manager = SourceManager()
    
    # RSS ì†ŒìŠ¤ ì¶”ê°€
    for source_name, rss_url in NEWS_SOURCES.items():
        manager.add_fetcher(
            RSSFetcher(source_name=source_name, rss_url=rss_url)
        )
    
    # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì¶”ê°€
    for keyword in NAVER_NEWS_KEYWORDS:
        manager.add_fetcher(
            NaverNewsFetcher(keyword=keyword)
        )
    
    return manager


# ============================================================
# ëª¨ë“ˆ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
# ============================================================

if __name__ == '__main__':
    print("=" * 60)
    print("source_fetcher.py (v2.1) ë‹¨ìœ„ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # SourceManager ìƒì„± ë° Fetcher ë“±ë¡
    manager = create_fetchers_from_config()
    
    # ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ (ê° ì†ŒìŠ¤ë‹¹ ìµœëŒ€ 3ê°œ)
    articles = manager.fetch_all_articles(limit_per_source=3)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“° ìˆ˜ì§‘ëœ ê¸°ì‚¬ ëª©ë¡:")
    print("-" * 60)
    for i, article in enumerate(articles, 1):
        print(f"\n[{i}] {article['title']}")
        print(f"    ì¶œì²˜: {article['source']}")
        print(f"    ë§í¬: {article['url'][:80]}...")
    
    print("\n" + "=" * 60)
    print("ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 60)