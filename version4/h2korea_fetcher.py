# h2korea_fetcher.py
"""
í•œêµ­ìˆ˜ì†Œì—°í•© ì •ê¸°ê°„í–‰ë¬¼ í¬ë¡¤ë§ ëª¨ë“ˆ (ì„ íƒì‚¬í•­)
"""

import requests
from bs4 import BeautifulSoup
from typing import List, Dict


def fetch_h2korea_publications(limit: int = 5) -> List[Dict[str, str]]:
    """
    í•œêµ­ìˆ˜ì†Œì—°í•© ì •ê¸°ê°„í–‰ë¬¼ í˜ì´ì§€ì—ì„œ ìµœì‹  ë°œí–‰ë¬¼ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        limit: ê°€ì ¸ì˜¬ ìµœëŒ€ ê°œìˆ˜
    
    Returns:
        List[Dict]: ë°œí–‰ë¬¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    
    url = "https://h2korea.or.kr/ko/index"
    print(f"[í•œêµ­ìˆ˜ì†Œì—°í•©] ì •ê¸°ê°„í–‰ë¬¼ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤: {url}")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        publications = []
        
        # ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ì„ íƒìë¥¼ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤
        # ì˜ˆì‹œ ì„ íƒì (ì‹¤ì œ ì‚¬ì´íŠ¸ í™•ì¸ í•„ìš”)
        items = soup.select('.publication-item')[:limit]
        
        for item in items:
            title_elem = item.select_one('.title')
            link_elem = item.select_one('a')
            
            if title_elem and link_elem:
                title = title_elem.get_text(strip=True)
                href = link_elem.get('href', '')
                
                # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if href and not href.startswith('http'):
                    from urllib.parse import urljoin
                    href = urljoin(url, href)
                
                publications.append({
                    'title': title,
                    'url': href,
                    'source': 'í•œêµ­ìˆ˜ì†Œì—°í•©'
                })
        
        print(f"  âœ… {len(publications)}ê°œì˜ ë°œí–‰ë¬¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return publications
        
    except Exception as e:
        print(f"  âŒ í•œêµ­ìˆ˜ì†Œì—°í•© í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


# ì—°ë„ë³„ ì´ìŠˆ ë¸Œë¦¬í•‘ PDF ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ (ì¶”ê°€ ê¸°ëŠ¥)
def download_h2korea_pdf(pdf_url: str, save_path: str) -> bool:
    """
    í•œêµ­ìˆ˜ì†Œì—°í•© PDF íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        pdf_url: PDF íŒŒì¼ URL
        save_path: ì €ì¥í•  ê²½ë¡œ
    
    Returns:
        bool: ë‹¤ìš´ë¡œë“œ ì„±ê³µ ì—¬ë¶€
    """
    try:
        print(f"  ğŸ“¥ PDF ë‹¤ìš´ë¡œë“œ ì¤‘: {pdf_url}")
        response = requests.get(pdf_url, timeout=30)
        response.raise_for_status()
        
        with open(save_path, 'wb') as f:
            f.write(response.content)
        
        print(f"  âœ… PDF ì €ì¥ ì™„ë£Œ: {save_path}")
        return True
        
    except Exception as e:
        print(f"  âŒ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


# ============================================================
# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
# ============================================================
if __name__ == '__main__':
    print("=" * 60)
    print("h2korea_fetcher.py ë‹¨ìœ„ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ë°œí–‰ë¬¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    publications = fetch_h2korea_publications(limit=3)
    
    if publications:
        print("\nğŸ“š ìˆ˜ì§‘ëœ ë°œí–‰ë¬¼:")
        for i, pub in enumerate(publications, 1):
            print(f"\n[{i}] {pub['title']}")
            print(f"    ë§í¬: {pub['url']}")
    else:
        print("\nâš ï¸  ë°œí–‰ë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("    ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    print("\n" + "=" * 60)
    print("ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 60)